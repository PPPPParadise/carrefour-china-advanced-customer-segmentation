
### Vectorize items

### 0. Initialization

import matplotlib
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import pandas as pd
from os.path import expanduser, join, abspath
from decimal import * 
import csv
from pyspark.sql import SQLContext
import os

import gensim
from gensim.models.word2vec import LineSentence
from gensim.models import Word2Vec
# from sklearn.manifold import TSNE


def step6_seg_pleasure_score_word2vec(spark,config):

    ### 1. Read Data
    # Read the data which generated by batch in step5
    csvfile = pd.read_csv(f'{config["temp_data_dir"]}/seg_item0_info.csv')

    ### 2. Data Transform
    # Transform the data with the following logic by removing the comma and separate the family_chn_name by space
    store_list = list(set(list(csvfile['store_code'])))
    csvfile['family_chn_name'] = [str(x).strip()  for x in  csvfile['family_chn_name'] ]
    csvfile_group = csvfile[csvfile.store_code==store_list[:1][0]].groupby(['member_card']).apply(lambda x: x.family_chn_name.tolist())
    for store in store_list[1:]:
        csvfile_group = csvfile_group.append(csvfile[csvfile.store_code==store].groupby(['member_card']).apply(lambda x: x.family_chn_name.tolist()))

    family_chn_name_df = pd.DataFrame(csvfile_group, columns = ['family_chn_name'])
    family_chn_name_df['family_chn_name_text'] = [str(x)[1:-1].replace("'","").replace('""','').replace(",","").replace("，","")  for x in  family_chn_name_df['family_chn_name'] ]

    family_chn_name_df['family_chn_name_text'].to_csv(f'{config["temp_data_dir"]}/family_chn_name_list.csv',index=False)

    ### 3. Modeling
    # 2. family_chn_name

    # feed to the model and construct the high dimensional areas
    source = LineSentence(f'{config["temp_data_dir"]}/family_chn_name_list.csv')
    model = Word2Vec(source, window=5, min_count=5, size=8, iter=50)
    # model.wv.vector_size
    model.save(f'{config["temp_data_dir"]}/family_chn_name.model')

    ### 4. Visualization

    # Visualize the model in 2-d space
    model = Word2Vec.load(f'{config["temp_data_dir"]}/family_chn_name.model')
    model.init_sims()
    vectors = model.wv.vectors_norm
    final_embeddings = vectors

    vocabs = model.wv.index2word
    labels_all = []
    for _ in vocabs:
        labels_all.append(_)


    ### 5. Set Pleasure Focus Vector

    # set the pairs of least pleasure-to-most pleasure
    pairs = [
        ('鞋垫', '土豆类干脆小食'),
        ('套装餐具', '甜点餐具'),
        ('清洁工具', '棋牌类游戏'),
        ('盒装肉菜', '汉堡及热狗'),
        ('盐&糖', '寿司／刺身'),
        ('普通猪肉', '进口冷冻冰淇淋甜点'),
        ('普通猪肉', '可乐'),
        ('圆领T', '圣诞装饰')
    ]

    label_embedding_dict = dict(zip(labels_all, final_embeddings))

    def normalize(v):
        norm = np.linalg.norm(v)
        if norm == 0:
            return v
        return v / norm

    # construct the finect vector by averaging the pairs
    pleasure_vectors = [label_embedding_dict[_2] - label_embedding_dict[_1] for _1, _2 in pairs]

    pleasure_vector = [normalize(v) for v in pleasure_vectors]

    pleasure_vector = np.mean(pleasure_vector, axis=0)

    ### 6. Calculate the pleasure amplitude

    # calculate the amplitude for all the family by projecting it to the finest vector
    selected_labels, selected_final_embeddings = list(zip(*[
        _ for _ in (zip(labels_all, final_embeddings))
        #if _[0].endswith(target)
    ]))

    amplitudes = [np.dot(pleasure_vector, v) for v in selected_final_embeddings]

    family_scores = sorted(list(zip(selected_labels, amplitudes)), key=lambda _: _[1], reverse=True)

    # Normalize the scroes from 0 to 1
    df_export = pd.DataFrame(family_scores, columns=['family', 'score'])
    df_export['n_score'] = (df_export.score - min(df_export.score)) / (max(df_export.score) - min(df_export.score))


    # df_export.to_csv(f'{config["temp_data_dir"]}/df_pleasure_score.csv', index=False, encoding='utf-8')

    ### 7. Export to hive

    # df_export = pd.read_csv("../../data/df_pleasure_score.csv")
    
    # export the data to hive
    sql_check = f"""
    select 
        family_code,
        family_name,
        family_chn_name as family_name_cn
    from {config['database_name']}.family_code_info
    """
    family_code_info = spark.sql(sql_check)
    print(sql_check)
    family_code_info_df = family_code_info.toPandas()

    df_export.rename({'family': 'family_name_cn'}, axis=1,inplace=True)

    seg_pleasure_score_df = pd.merge(family_code_info_df, \
                                    df_export[['n_score','score','family_name_cn']]\
                                , how='left', on=['family_name_cn'])

    seg_pleasure_score_df.fillna(0,inplace=True)

    # spark_df = spark.createDataFrame(seg_pleasure_score_df)

    # spark_df.write.mode('overwrite').saveAsTable(f"{config['database_name']}.seg_pleasure_score")



    seg_pleasure_score_df.to_csv(f'{config["temp_data_dir"]}/seg_step6.csv', index = False)
    linux_command_result = os.system(f'hadoop fs -put -f {config["temp_data_dir"]}/seg_step6.csv  {config["hdfs_dir"]}/seg_step6.csv ')
    if linux_command_result != 0:
        raise Exception("""upload seg_step6.csv to hdfs failed. 
    (due to the result is large, we upload the result in csv format first, then insert into the database.)
        """)

    sqlContext = SQLContext(spark)
    spark_df = sqlContext.read.format('com.databricks.spark.csv').options(header='true').load('seg_step6.csv')
    spark_df.write.mode('overwrite').saveAsTable(f"{config['database_name']}.seg_pleasure_score")