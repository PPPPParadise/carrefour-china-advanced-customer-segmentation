
### Vectorize items

### 0. Initialization

import matplotlib
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import pandas as pd
from os.path import expanduser, join, abspath
from decimal import * 
import csv
import time
from pyspark.sql import SQLContext
import os

import gensim
from gensim.models.word2vec import LineSentence
from gensim.models import Word2Vec

def step5_seg_finest_amplitude_word2vec(spark,config):
    ### 1. Read Data

    # The process will be generated in batch to avoid the the memory limit
    # Sampling is applied to reduce the speed of tuning
    # From: vartefact.trxns_from_kudu
    # To: csv seg_item0_info.csv
    # Usage: Retrieve the data from data lake to have information 
    #        include item, member_card and brand_family

    # get store list
    # the store picking rule is one max and one mean by each territory + 114(gubei)
    sql = f"""
    select
        territory_code,
        store_code,
        sum(sales_amt) as sales_amount
    from {config['database_name']}.trxns_from_kudu
    group by territory_code,store_code
    """
    store_sdf = spark.sql(sql)
    store_df = store_sdf.toPandas()
    territory_code_list = ['T1','T2','T3','T4','T5','T6','T7']

    store_list = []
    for i in territory_code_list:
        each_territory_df = store_df[store_df.territory_code == i].copy()
        each_territory_df.sort_values(['sales_amount'],inplace=True)
        store_list.extend(each_territory_df[each_territory_df.sales_amount>=
            each_territory_df.sales_amount.median()][:1]['store_code'].values)
    #     print(each_territory_df)
        store_list.extend(each_territory_df[each_territory_df.sales_amount==
            each_territory_df.sales_amount.max()][:1]['store_code'].values)
    store_list.append('114')
    store_list = list(set(store_list))

    # Retrieve the data from data lake to have information include item, member_card and brand_family 
    # The csv will be generated in batch to avoid the the memory limit
    # Sampling is applied to reduce the speed of tuning
    def generate_csv():
        header = True
        for store in store_list:
            print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))
            print(store)
            for member in range(0,10):
                sql = f"""
                    select
                        trxns.item_id,
                        trxns.store_code,
                        trxns.member_card,
                        item.brand_family_ as brand_family,
                        item.family_chn_name,
                        concat(trxns.item_brand, '_', trxns.family_code) as brand_family_code
                    from 
                    (
                        select 
                            item_id,
                            store_code,
                            member_card,
                            item_brand,
                            family_code
                        from {config['database_name']}.trxns_from_kudu
                        where 
                            store_code = '{store}'
                            and
                            substr(member_card, -1) = '{member}'
                    ) trxns
                    left join {config['database_name']}.seg_item_detail_1 item
                        on trxns.item_id = item.item_id
                """
                print(member)
                trxns = spark.sql(sql)
                trxns_df = trxns.toPandas()
                if header:
                    # print(sql)
                    trxns_df.to_csv(f'{config["temp_data_dir"]}/seg_item0_info.csv',mode='w',index=False, header=header)
                else:
                    trxns_df.to_csv(f'{config["temp_data_dir"]}/seg_item0_info.csv',mode='a',index=False, header=header)
                header = False
                
    generate_csv()

    # Read the data which generated by batch
    csvfile = pd.read_csv(f'{config["temp_data_dir"]}/seg_item0_info.csv')

    ### 2. Data Transform


    # Transform the data with the following logic by removing the comma and separate the brand_family by space
    store_list = list(set(list(csvfile['store_code'])))
    csvfile['brand_family'] = [str(x).strip()  for x in  csvfile['brand_family'] ]
    csvfile_group = csvfile[csvfile.store_code==store_list[:1][0]].groupby(['member_card']).apply(lambda x: x.brand_family.tolist())
    for store in store_list[1:]:
        csvfile_group = csvfile_group.append(csvfile[csvfile.store_code==store].groupby(['member_card']).apply(lambda x: x.brand_family.tolist()))

    brand_family_df = pd.DataFrame(csvfile_group, columns = ['brand_family'])
    # Clean the data
    brand_family_df['brand_family_text'] = [str(x)[1:-1].replace("'","").replace('"','').replace(",","")  for x in  brand_family_df['brand_family'] ]

    brand_family_df['brand_family_text'].to_csv(f'{config["temp_data_dir"]}/brand_family_list.csv',index=False)


    ### 3. Modeling
    # 1. brand_family

    # feed to the model and construct the high dimensional areas
    source = LineSentence(f'{config["temp_data_dir"]}/brand_family_list.csv')
    model = Word2Vec(source, window=5, min_count=15, size=32, iter=50)
    # model.wv.vector_size
    model.save(f'{config["temp_data_dir"]}/brand_family.model')

    ### 4. Visualize

    # Visualize the model in 2-d space
    model = Word2Vec.load(f'{config["temp_data_dir"]}/brand_family.model')
    model.init_sims()
    vectors = model.wv.vectors_norm
    final_embeddings = vectors

    vocabs = model.wv.index2word

    labels_all = []
    for _ in vocabs:
        labels_all.append(_)


    ### 5. Set Finest Vector

    # set the pairs of cheap-to-finest
    # ================================================
    # Attention:
    #   Chose of paris are very experience based, 
    #.  for further improvment, asking more experts for
    #   help to explor the true market position of brands.
    pairs = [
        ('皓齿健_牙膏', '云南白药_牙膏'),
        ('皓齿健_牙膏', '欧乐B_牙膏')
        # ('心相印_面纸', '滴露_面纸'),
        # ('好吉利_面纸', '维达_面纸'),
        # ('福临门_油', '贝蒂斯_油'),
        # ('金龙鱼_油', '欣奇典_油'),
        # ('散装蛋_鸡蛋', '圣迪乐_鸡蛋'),
        # ('美加净_脸部保养品(非专柜)', '欧莱雅_脸部保养品(非专柜)'),
        # ('士力架_巧克力', '费列罗_巧克力'),
        # ('心相印_婴儿湿巾', '嗳呵_婴儿湿巾'),
        # ('青岛_国产啤酒', '科罗娜_进口啤酒'),
        # ('彩虹_杀爬虫用品', '雷达_杀爬虫用品')
    ]

    label_embedding_dict = dict(zip(labels_all, final_embeddings))

    def normalize(v):
        norm = np.linalg.norm(v)
        if norm == 0:
            return v
        return v / norm

    # construct the finect vector by averaging the pairs
    finest_vectors = [label_embedding_dict[_2] - label_embedding_dict[_1] for _1, _2 in pairs]
    finest_vector = [normalize(v) for v in finest_vectors]
    finest_vector = np.mean(finest_vector, axis=0)


    ### 4. Calculate the finest amplitude

    # calculate the amplitude for all the brand_family by projecting it to the finest vector
    amplitudes = [np.dot(finest_vector, v) for v in final_embeddings]

    df_amplitude = pd.DataFrame(sorted(list(zip(labels_all, amplitudes)), key=lambda _: _[1], reverse=True), columns=['brand_family_', 'amplitude'])
    df_amplitude.dropna(axis=0,inplace=True)
    df_amplitude.brand_family_.astype(str)


    ### 5. Export to Hive
    # export the data to hive
    # spark_df = spark.createDataFrame(df_amplitude)

    # spark_df.write.mode('overwrite').saveAsTable(f"{config['database_name']}.seg_finest_amplitude")

    df_amplitude.to_csv(f'{config["temp_data_dir"]}/seg_step5.csv', index = False)
    linux_command_result = os.system(f'hadoop fs -put -f {config["temp_data_dir"]}/seg_step5.csv  {config["hdfs_dir"]}/seg_step5.csv ')
    if linux_command_result != 0:
        raise Exception("""upload seg_step5.csv to hdfs failed. 
    (due to the result is large, we upload the result in csv format first, then insert into the database.)
        """)

    sqlContext = SQLContext(spark)
    spark_df = sqlContext.read.format('com.databricks.spark.csv').options(header='true').load('seg_step5.csv')
    spark_df.write.mode('overwrite').saveAsTable(f"{config['database_name']}.seg_finest_amplitude")